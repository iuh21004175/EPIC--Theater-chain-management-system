document.addEventListener('DOMContentLoaded', function() {
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const btnStart = document.getElementById('btnStartCapture');
    const faceNotify = document.getElementById('faceNotify') || createNotifyDiv();
    // disable start until models loaded
    btnStart.disabled = true;
    function createNotifyDiv() {
        let div = document.createElement('div');
        div.id = 'faceNotify';
        div.className = 'mt-4 w-full text-center text-base font-semibold';
        btnStart.parentNode.insertBefore(div, btnStart);
        return div;
    }

    let stream = null;
    let intervalOfCapture = null;
    let modelsLoaded = false;
    let isAnalyzing = false;
    let lastDetectionBox = null;
    const lastDetections = [];

    // ƒê·ªìng b·ªô k√≠ch th∆∞·ªõc canvas internal v·ªõi k√≠ch th∆∞·ªõc hi·ªÉn th·ªã (CSS) c·ªßa element
    function setCanvasSizeToElement(canvas, element) {
        const rect = element.getBoundingClientRect();
        // G√°n internal pixel size = CSS display size ƒë·ªÉ toDataURL tr·∫£ ·∫£nh ƒë√∫ng k√≠ch th∆∞·ªõc hi·ªÉn th·ªã
        canvas.width = Math.round(rect.width);
        canvas.height = Math.round(rect.height);
        canvas.style.width = `${rect.width}px`;
        canvas.style.height = `${rect.height}px`;
    }

    // Scale bounding box from video native pixels -> overlay CSS pixels and handle mirror
    function scaleBoxToDisplay(box) {
        if (!video.videoWidth || !video.videoHeight) return box;
        const rect = overlay.getBoundingClientRect();
        const scaleX = rect.width / video.videoWidth;
        const scaleY = rect.height / video.videoHeight;
        const computed = getComputedStyle(video);
        const isFlipped = computed.transform && computed.transform.includes('-1');
        let x = box.x * scaleX;
        if (isFlipped) {
            // flip horizontally: newX = displayWidth - (box.x + box.width) * scaleX
            x = rect.width - (box.x + box.width) * scaleX;
        }
        return {
            x: x,
            y: box.y * scaleY,
            width: box.width * scaleX,
            height: box.height * scaleY
        };
    }

    /** üîπ T·∫£i m√¥ h√¨nh */
    async function loadModels() {
        await Promise.all([
            faceapi.nets.tinyFaceDetector.loadFromUri('./models'),
            faceapi.nets.faceLandmark68Net.loadFromUri('./models')
        ]);
        modelsLoaded = true;
        // enable start button when models ready
        btnStart.disabled = false;
        faceNotify.textContent = 'M√¥ h√¨nh ƒë√£ s·∫µn s√†ng.';
        faceNotify.className = 'mt-4 w-full text-center text-base font-semibold text-green-600';
        startCamera();
    }

    /** üîπ B·∫≠t camera */
    async function startCamera() {
        try {
            stream = await navigator.mediaDevices.getUserMedia({
                video: { width: { ideal: 1280 }, height: { ideal: 720 }, facingMode: 'user' }
            });
            video.srcObject = stream;
            video.addEventListener('loadedmetadata', () => {
                // Set overlay canvas size to match displayed video element size (CSS pixels)
                setCanvasSizeToElement(overlay, video);
                drawLoop(); // üî• B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p v·∫Ω m∆∞·ª£t
            });
        } catch (error) {
            alert("Kh√¥ng th·ªÉ truy c·∫≠p camera. Vui l√≤ng ki·ªÉm tra quy·ªÅn truy c·∫≠p.");
            console.error(error);
        }
    }

    /** üîπ V·∫Ω video v√† khung khu√¥n m·∫∑t */
    async function drawLoop() {
        const ctx = overlay.getContext('2d', { willReadFrequently: true });
        const rect = overlay.getBoundingClientRect();
        ctx.clearRect(0, 0, rect.width, rect.height);
        // drawImage to CSS-size canvas so overlay and capture match display
        ctx.drawImage(video, 0, 0, rect.width, rect.height);

        if (modelsLoaded && !isAnalyzing) {
            isAnalyzing = true;
            faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
                .withFaceLandmarks()
                .then(detection => {
                    if (detection && detection.detection) {
                        const box = detection.detection.box;
                        // scale to overlay display coords (and handle mirror)
                        lastDetectionBox = scaleBoxToDisplay(box);
                    }
                })
                .finally(() => (isAnalyzing = false));
        }

        // üî≤ V·∫Ω khung khu√¥n m·∫∑t n·∫øu c√≥
        if (lastDetectionBox) {
            ctx.save();
            ctx.strokeStyle = '#00e676';
            ctx.lineWidth = 3;
            ctx.strokeRect(lastDetectionBox.x, lastDetectionBox.y, lastDetectionBox.width, lastDetectionBox.height);
            ctx.restore();
        }

        requestAnimationFrame(drawLoop);
    }

    /** üß© Ki·ªÉm tra ·∫£nh ƒë·∫°t chu·∫©n ƒë·ªÉ d√πng v·ªõi ArcFace (k√®m ph√°t hi·ªán che m·∫∑t) */
    async function isFaceImageValid(canvas) {
        if (!modelsLoaded)
            return { valid: false, message: '‚è≥ Ch∆∞a t·∫£i xong m√¥ h√¨nh nh·∫≠n di·ªán.' };

        // th·ª≠ nhi·ªÅu c·∫•u h√¨nh ƒë·ªÉ robust h∆°n
        const tryOptions = [
            new faceapi.TinyFaceDetectorOptions({ inputSize: 512, scoreThreshold: 0.3 }),
            new faceapi.TinyFaceDetectorOptions({ inputSize: 416, scoreThreshold: 0.35 }),
            new faceapi.TinyFaceDetectorOptions({ inputSize: 320, scoreThreshold: 0.25 })
        ];

        try {
            let detection = null;
            let detections = null;

            // debug: l∆∞u ·∫£nh ra console ƒë·ªÉ ki·ªÉm tra th·ªß c√¥ng n·∫øu c·∫ßn
            try { console.debug('Debug canvas dataURL (first 120 chars):', (canvas.toDataURL().slice(0,120))); } catch(e){}

            // Th·ª≠ detectSingleFace nhanh tr∆∞·ªõc, fallback sang detectAllFaces n·∫øu kh√¥ng c√≥
            for (const options of tryOptions) {
                try {
                    detection = await faceapi.detectSingleFace(canvas, options).withFaceLandmarks();
                } catch (e) {
                    detection = null;
                }
                if (detection && detection.detection) break;
            }

            // n·∫øu detectSingleFace kh√¥ng th√†nh c√¥ng, th·ª≠ detectAllFaces (ƒë·ªÉ bi·∫øt c√≥ >1 face)
            if (!detection) {
                for (const options of tryOptions) {
                    try {
                        detections = await faceapi.detectAllFaces(canvas, options).withFaceLandmarks();
                    } catch (e) {
                        detections = null;
                    }
                    if (detections && detections.length > 0) {
                        if (detections.length > 1) {
                            return { valid: false, message: '‚ö†Ô∏è C√≥ nhi·ªÅu khu√¥n m·∫∑t trong khung. H√£y ƒë·ªÉ 1 ng∆∞·ªùi duy nh·∫•t.' };
                        }
                        detection = detections[0];
                        break;
                    }
                }
            }

            console.debug('Final detection:', !!detection, detection);

            if (!detection || !detection.detection) {
                return { valid: false, message: '‚ùå Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t. H√£y nh√¨n th·∫≥ng v√†o camera ho·∫∑c th·ª≠ gi·∫£m threshold inputSize.' };
            }

            const box = detection.detection.box;
            const landmarks = detection.landmarks;

            // c√°c ki·ªÉm tra ti·∫øp theo (ƒë·ªô s√°ng, n√©t, t·ª∑ l·ªá, che m·∫∑t...) gi·ªØ nguy√™n nh∆∞ c≈©
            const brightness = getAverageBrightness(canvas);
            if (brightness < 70) return { valid: false, message: 'üí° ·∫¢nh qu√° t·ªëi. H√£y ch·ª•p ·ªü n∆°i s√°ng h∆°n.' };
            if (brightness > 200) return { valid: false, message: 'üí° ·∫¢nh qu√° s√°ng. H√£y gi·∫£m √°nh s√°ng ho·∫∑c tr√°nh ng∆∞·ª£c s√°ng.' };

            const sharpness = getFaceSharpness(canvas, box);
            const minSharpness = 180;
            if (sharpness < minSharpness) return { valid: false, message: 'üîç ·∫¢nh b·ªã m·ªù. H√£y gi·ªØ y√™n khu√¥n m·∫∑t khi ch·ª•p.' };

            const faceRatio = (box.width * box.height) / (canvas.width * canvas.height);
            if (faceRatio < 0.35) return { valid: false, message: 'üìè Khu√¥n m·∫∑t qu√° nh·ªè. H√£y ti·∫øn l·∫°i g·∫ßn h∆°n.' };
            if (faceRatio > 0.8) return { valid: false, message: 'üìè Khu√¥n m·∫∑t qu√° g·∫ßn. H√£y l√πi ra m·ªôt ch√∫t.' };

            const leftEye = landmarks.getLeftEye();
            const rightEye = landmarks.getRightEye();
            const eyeSlope = Math.abs(leftEye[0].y - rightEye[0].y);
            if (eyeSlope > 10) return { valid: false, message: '‚ÜîÔ∏è M·∫∑t nghi√™ng qu√° m·ª©c. H√£y ch·ªânh l·∫°i cho th·∫≥ng.' };

            const faceCenterX = box.x + box.width / 2;
            const faceCenterY = box.y + box.height / 2;
            const frameCenterX = canvas.width / 2;
            const frameCenterY = canvas.height / 2;
            if (Math.abs(faceCenterX - frameCenterX) > canvas.width * 0.2)
                return { valid: false, message: 'üì∏ Khu√¥n m·∫∑t l·ªách sang m·ªôt b√™n. H√£y cƒÉn gi·ªØa.' };
            if (Math.abs(faceCenterY - frameCenterY) > canvas.height * 0.2)
                return { valid: false, message: 'üì∏ Khu√¥n m·∫∑t qu√° cao ho·∫∑c th·∫•p trong khung h√¨nh.' };

            // ph√°t hi·ªán che m·∫∑t c∆° b·∫£n
            const nose = landmarks.getNose();
            const mouth = landmarks.getMouth();
            const jaw = landmarks.getJawOutline();
            const mouthTopY = mouth[0].y;
            const mouthBottomY = mouth[mouth.length - 1].y;
            const mouthHeight = mouthBottomY - mouthTopY;
            if (mouthHeight < 4) return { valid: false, message: 'üò∑ Kh√¥ng th·∫•y r√µ v√πng mi·ªáng. C√≥ th·ªÉ b·∫°n ƒëang ƒëeo kh·∫©u trang ho·∫∑c b·ªã che m·∫∑t.' };

            // ·ªïn ƒë·ªãnh chuy·ªÉn ƒë·ªông (gi·ªØ logic hi·ªán t·∫°i)
            lastDetections.push({ x: box.x, y: box.y, sharpness });
            if (lastDetections.length > 5) lastDetections.shift();
            if (lastDetections.length >= 3) {
                const dx = Math.abs(lastDetections[4].x - lastDetections[0].x);
                const dy = Math.abs(lastDetections[4].y - lastDetections[0].y);
                const motion = Math.sqrt(dx * dx + dy * dy);
                const avgSharp = lastDetections.reduce((a, b) => a + b.sharpness, 0) / lastDetections.length;
                if (motion > 8) return { valid: false, message: 'üì∑ Khu√¥n m·∫∑t ƒëang di chuy·ªÉn. H√£y gi·ªØ y√™n.' };
                if (avgSharp < minSharpness) return { valid: false, message: 'üì∑ ·∫¢nh b·ªã m·ªù do chuy·ªÉn ƒë·ªông. H√£y gi·ªØ y√™n khu√¥n m·∫∑t.' };
            }

            return { valid: true, message: '‚úÖ ·∫¢nh ƒë·∫°t chu·∫©n, c√≥ th·ªÉ d√πng cho nh·∫≠n di·ªán ArcFace.' };
        } catch (err) {
            console.error('Face detection error:', err);
            return { valid: false, message: '‚ùå L·ªói khi nh·∫≠n di·ªán khu√¥n m·∫∑t, ki·ªÉm tra console ƒë·ªÉ bi·∫øt chi ti·∫øt.' };
        }
    }



    /** üîπ ƒê·ªô n√©t khu√¥n m·∫∑t */
    function getFaceSharpness(canvas, box) {
        const ctx = canvas.getContext('2d', { willReadFrequently: true });
        const faceData = ctx.getImageData(box.x, box.y, box.width, box.height);
        const { data, width, height } = faceData;
        const gray = new Float32Array(width * height);

        for (let i = 0; i < data.length; i += 4)
            gray[i / 4] = 0.299 * data[i] + 0.587 * data[i + 1] + 0.114 * data[i + 2];

        const kernel = [-1, -1, -1, -1, 8, -1, -1, -1, -1];
        let variance = 0;
        for (let y = 1; y < height - 1; y++) {
            for (let x = 1; x < width - 1; x++) {
                let sum = 0;
                for (let ky = -1; ky <= 1; ky++) {
                    for (let kx = -1; kx <= 1; kx++) {
                        const idx = (y + ky) * width + (x + kx);
                        sum += gray[idx] * kernel[(ky + 1) * 3 + (kx + 1)];
                    }
                }
                variance += sum * sum;
            }
        }
        return variance / (width * height);
    }

    /** üîπ ƒê·ªô s√°ng trung b√¨nh */
    function getAverageBrightness(canvas) {
        const ctx = canvas.getContext('2d', { willReadFrequently: true });
        const { data } = ctx.getImageData(0, 0, canvas.width, canvas.height);
        let sum = 0;
        for (let i = 0; i < data.length; i += 4)
            sum += 0.299 * data[i] + 0.587 * data[i + 1] + 0.114 * data[i + 2];
        return sum / (data.length / 4);
    }

    /** üîπ Khi nh·∫•n "B·∫Øt ƒë·∫ßu" */
    loadModels();
    btnStart.addEventListener('click', function() {
        if (intervalOfCapture) clearInterval(intervalOfCapture);
        btnStart.disabled = true;
        btnStart.style.display = 'none';
        faceNotify.textContent = 'ƒêang l·∫•y d·ªØ li·ªáu...';
        faceNotify.className = 'mt-4 w-full text-center text-base font-semibold text-blue-600';

        intervalOfCapture = setInterval(async () => {
            if (isAnalyzing) return;

            const offCanvas = document.createElement('canvas');
            // Make capture canvas exactly the same displayed size as overlay/video
            setCanvasSizeToElement(offCanvas, video);
            const ctx = offCanvas.getContext('2d', { willReadFrequently: true });
            // offCanvas is not in DOM, so use its width/height properties (set by setCanvasSizeToElement)
            const w = offCanvas.width || video.clientWidth || 640;
            const h = offCanvas.height || video.clientHeight || 480;
            console.debug('Capture canvas size:', w, h, 'video native:', video.videoWidth, video.videoHeight);
            ctx.drawImage(video, 0, 0, w, h);

            const result = await isFaceImageValid(offCanvas);
            console.debug('isFaceImageValid =>', result);
            faceNotify.textContent = result.message;
            faceNotify.className = result.valid
                ? 'mt-4 w-full text-center text-base font-semibold text-green-700'
                : 'mt-4 w-full text-center text-base font-semibold text-red-600';

            if (result.valid) {
                clearInterval(intervalOfCapture);

                // ensure last frame drawn to offCanvas: wait one rAF
                await new Promise(r => requestAnimationFrame(r));

                // Convert base64 to Blob BEFORE stopping the camera
                const dataUrl = offCanvas.toDataURL('image/jpeg');
                if (!dataUrl || dataUrl.length < 100) {
                    console.error('Captured dataURL is empty or too small', dataUrl && dataUrl.length);
                }

                const byteString = atob(dataUrl.split(',')[1]);
                const mimeString = dataUrl.split(',')[0].split(':')[1].split(';')[0];
                const ab = new ArrayBuffer(byteString.length);
                const ia = new Uint8Array(ab);
                for (let i = 0; i < byteString.length; i++) {
                    ia[i] = byteString.charCodeAt(i);
                }
                const blob = new Blob([ab], { type: mimeString });

                // stop video AFTER we have the dataURL/blob
                if (stream) {
                    stream.getTracks().forEach(track => track.stop());
                }
                video.srcObject = null;

                const formData = new FormData();
                formData.append('image', blob, 'face.jpg');
                formData.append('staff_id', window.staffId || document.body.dataset.staffId);

                console.debug('Uploading image blob, size:', blob.size, 'canvas:', offCanvas.width, offCanvas.height);
                const response = await fetch(document.body.dataset.url + '/api/cham-cong/dang-ky-khuon-mat', {
                    method: 'POST',
                    body: formData
                });
                const result = await response.json();
                if (result.success) {
                    faceNotify.textContent = '‚úÖ ƒêƒÉng k√Ω khu√¥n m·∫∑t th√†nh c√¥ng!';
                    faceNotify.className = 'mt-4 w-full text-center text-base font-semibold text-green-700';
                } else {
                    faceNotify.textContent = '‚ùå ƒêƒÉng k√Ω khu√¥n m·∫∑t th·∫•t b·∫°i, Khu√¥n m·∫∑t kh√¥ng h·ª£p l·ªá.';
                    faceNotify.className = 'mt-4 w-full text-center text-base font-semibold text-red-700';  
                }
            }
        }, 500);
    });
});
